---
title: "NLP_Project"
author: "Jack, Yeonji, Alexi, Minxin"
date: "4/25/2018"
output: html_document
---

## Project Overview

###Context
The data set we chose to analyze is students’ online reflections on the e-Portfolio system at the NYU College of Dentistry (NYUCD). The system was designed for NYU dentistry students to do a self-assessment of their academic and professional knowledge and skills year by year. Each year students were required to write an online reflection assignment including survey and self-reflection. The e-portfolio for each student includes statements (personal, private personal, self-reflection on becoming a professional, private self-reflection on becoming a professional, ethic reflection, professional progress), course reflections, competency reflections, MDR/competency exams attempts, file uploads, and knowledge inventory questionnaires.

To be specific, knowledge-inventory-questionnaires (KIQ) tab has two questions to answer about the 48 core competencies that NYUCD emphasized. 
  • Rating data; they rated how much they thought they had each competency with 6-Likert scale (6 is the most likely).
  • Text data; they wrote their resolution or plan for each competency question
  
   I feel I will enhance or maintain this skill in the following activity or program
  
For instance, there are 2017 graduating students who submitted their answers to KIQ tab on the e-Portfolio system from 2013/2014 (Y1) to 2016/2017 (Y4). If one student answers all the questions without any missing response in Y1, there would be 96 data which consists of 48 rating data and 48 text data. If she/he answered all gain in Y4, there would be another 96 data, which can be accumulated and matched with the Y1 data set. 

###Data
Finally, we chose our data set as ### dental students who graduated 2017 and submitted their reflections on knowledge-inventory-questionnaires (KIQ) tab on the e-Portfolio systems for both Y1 and Y4.

Specific decisions about the data we make are as follows:
  (a) *Text data* on KIQ on the system
      This data indicates each student's resolution and plan to maintain or cultivate competency based on their                reflection, which would include their sentiments embedded in the words. 
  
  (b) *Two questions* among 48 competency questions on KIQ
      At first, there were the total 48 competency questions that each student answered in KIQ, which made a lot of            students just copy and paste their answers from the first question to the final questions. In addition, there are        some questions who seem overlapped and similar with the other questions in terms of the contents (e.g. Q34. I feel       competent in my ability to manage partial edentulism, Q35. I feel competent in my ability to manage complete             edentulism.). Thus, we decided to focus on the two important questions which indicate core competencies that dental       students should develop and may include more sentimental words: 
       • *[understanding patient]* Q3. I feel competent in my ability to understand patient expectations and achieve               their needs.
       • *[application for treatment]* Q22. I feel competent in my ability to apply the relevant and valid evidence to             the procedure you are/or plan to perform and develop an appropriate treatment plan.

  (c) Reflections made by *both Y1 and Y4*
      As well as to look at each year's data set individually, we decided to figure out the change of the sentiment words       between Y1 and Y4 that students showed. We expected that identifying the changes of the sentiment words between          their first year and their graduation year would be helpful to figure out which aspects, concepts, activities are        related to their sentiment.
 
###Questions
What kinds of words are embedded in students' resolution to develop their competency?
What kinds of sentimental words are invovled in students' resolution to develop their competency?
How does sentimental words change from the first year to their graduate year?


###Procedues
Overall procedures we conducted are as follows:

1. Walk through the data set
  (a) Get the data.
  (b) Look at the data set closely to get a sense of what the data can tell about learning.
  (c) Decide to focus on “content” (language) ---> “NLP”

2. Make key decisions
  (a) Clarify the research questions 
  (b) Decide to conduct “sentimental analysis with n/gram” as well as basic content analysis including word frequency          among diverse NLP techniques
  (c) Finally select the data set for analysis to answer the questions.
  (d) Select the R package to conduct data analysis

3. Do pilot test with Y4 data set
  (a) Data Cleaning & Tidying
        • Data scraping for Y1 data using Web Scraper (Avi's help!)
        • Data cleaning for Y4 data set using Excel
        • Data cleaning & tidying
  (b)  Basic content analysis & interpretation
        • Word frequency & scatter plot
  (c)  Sentiment analysis & interpretation
        • Positive/negative sentiment analysis; finding specific sentiment words associated most with competency
        • SNA sentiment analysis; finding more specific sentiment words and their relationships per sentiment

4. Conduct analysis for both Y1 and Y4 data set
  (a) Data Cleaning & Tidying
        • Data scraping for Y1 data using Web Scraper
        • Data matching & cleaning for both Y1 and Y4 data set using Excel
        • Data cleaning & tidying
  (b) Basic analysis
        •	All word frequency
        •	Positive/Negative word frequency
        •	SNA (Overall or Pos/Neg)
  (c) Sentiment analysis
        • [Q1] Compare Year1 vs Year4
              • All Word Frequency per sentiment 
              •	Bi-gram Word Frequency per sentiment 
              •	All SNA per sentiment
              •	Bi-gram SNA per sentiment
        • [Q2] Compare Year1 vs Year4
              • All Word Frequency per sentiment 
              •	Bi-gram Word Frequency per sentiment 
              •	All SNA per sentiment
              •	Bi-gram SNA per sentiment

5. Integrate and examine the results all together

6. Suggest conclusions and implications


## Data Cleaning & Tidying

```{r}
#install tidytext package for tidying
install.packages("tidytext")
#install readxl package for loading an excel file
install.packages("readxl")
install.packages("rowr")
install.packages("plyr")
install.packages("igraph")
install.packages("ggraph")
```

```{r}
library(tidytext)
library(stringr)
library(readxl)
library(ggplot2)
library(tidyr)
library(plyr)
library(rowr)
library(dplyr)
library(igraph)
library(ggraph)
```



```{r}
#load the first sheet of the excel file, which contains the data of all questions and students
dentalDataAll <- read_excel("DentalDataKIQ2017D1&D4P(anonymized).xlsx", sheet = 1)
```


```{r}
#filter out questions other than Q3 and Q22 for both years
##found out that there are missing students in Y1
##combined data frames for both years by column and added a flag column for compairson

dentalDataD1 <- filter(dentalDataAll, RefYear == "D1" & (kiqQs == "cp3" | kiqQs == "cp22"))
colnames(dentalDataD1)[colnames(dentalDataD1)=="RandomID"] <- "RandomIDY1"
dentalDataD4 <- filter(dentalDataAll, RefYear == "D4" & (kiqQs == "cp3" | kiqQs == "cp22"))
colnames(dentalDataD4)[colnames(dentalDataD4)=="RandomID"] <- "RandomIDY4"
dentalDataD1D4 <- cbind.fill(dentalDataD1, dentalDataD4, c(), fill = 0)
colnames(dentalDataD1D4)[colnames(dentalDataD1D4)=="x[[i]]"] <- "flag"

```

```{r}
#use for loop and if condition to find out the students that are not presented in Year 1
dentalDataD1D4$RandomIDY1 <- as.character(dentalDataD1D4$RandomIDY1)
dentalDataD1D4$RandomIDY4 <- as.character(dentalDataD1D4$RandomIDY4)

for(i in c(1:734)) {
  for(j in c(1:734)) {
  if(dentalDataD1D4$RandomIDY4[i] == dentalDataD1D4$RandomIDY1[j]) {
  dentalDataD1D4$flag[i] <- 1
  }
  }
}
```

```{r}
#filter out the students that are not presented in Year 1
##now both years have the same observations 

dentalDataD4 <- cbind(dentalDataD4,flag=dentalDataD1D4$flag)

dentalDataD4 <- filter(dentalDataD4, flag != 0)
```

```{r}
#test to see if the students match betwen both years
test <- cbind(dentalDataD1$RandomIDY1,dentalDataD4$RandomIDY4)

names(dentalDataD4) <- c("TypeY4", "GradYearY4", "RefYearY4","PracGroupY4","RandomIDY4", "kiqQsY4","kiqAns1Y4","kiqAns2Y4","flag")

#remove rows with missing value
CompareD1D4 <- cbind(dentalDataD1,select(dentalDataD4, -flag))
CompareD1D4 <- filter(CompareD1D4, !is.na(kiqAns2))
CompareD1D4 <- filter(CompareD1D4, !is.na(kiqAns2Y4))

#remove rows with same values for year 1 and year 4
CompareD1D4 <- CompareD1D4[!duplicated(CompareD1D4[,c('kiqAns2', 'kiqAns2Y4')]),]

D1 <- select(CompareD1D4,Type:kiqAns2)
D4 <- select(CompareD1D4,TypeY4:kiqAns2Y4)

names(D4) <- c("Type", "GradYear", "RefYear","PracGroup","RandomID", "kiqQs","kiqAns1","kiqAns2")
colnames(D1)[colnames(D1)=="RandomIDY1"] <- "RandomID"

#it contains the students only who answered both years and with different answers
#also, it contains only the students who are presented in both years
Y1Y4 <- rbind(D1,D4)

```

```{r}
#text tidying, follow the one-token-per-row rule
#remove stop words

Y1Y4Tokens <- unnest_tokens(Y1Y4,word,kiqAns2)

data(stop_words)

Y1Y4Clean <- Y1Y4Tokens %>%
  anti_join(stop_words)


```


```{r}
#separate the overall data frame (Y1Y4Clean) for the needs of comparing different questions between different years

Y1 <- filter(Y1Y4Clean, RefYear == "D1")
Y1Q1 <- filter(Y1Y4Clean, RefYear == "D1" & kiqQs == "cp3")
Y1Q2 <- filter(Y1Y4Clean, RefYear == "D1" & kiqQs == "cp22")
Y4 <- filter(Y1Y4Clean, RefYear == "D4")
Y4Q1 <- filter(Y1Y4Clean, RefYear == "D4" & kiqQs == "cp3")
Y4Q2 <- filter(Y1Y4Clean, RefYear == "D4" & kiqQs == "cp22")
```


##Basic analysis
```{r}
#overall word count including both years and both questions
##filter out the words that appear less than 100 times
##sorted by amount
count
Y1Y4Clean %>%
  count(word, sort = TRUE) %>%
  filter(n > 100) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip()


```

```{r}
#overall positive and negative words
bing_word_counts <- Y1Y4Clean %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  ungroup()

bing_word_counts
```

```{r}
#overall positive and negative words visualization
bing_word_counts %>%
  group_by(sentiment) %>%
  top_n(10) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(y = "Contribution to sentiment",
       x = NULL) +
  coord_flip()
```

```{r}
#overall bigram count
Y1Y4Tokens_bigrams <- Y1Y4Tokens %>%
  unnest_tokens(bigram, word, token = "ngrams", n = 2)

Y1Y4Tokens_bigrams %>%
  count(bigram, sort = TRUE)


```

```{r}
#overall bigram count that filtered out stop words
bigrams_separated <- Y1Y4Tokens_bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ")

bigrams_filtered <- bigrams_separated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)

#new bigram counts:
bigram_counts <- bigrams_filtered %>% 
  count(word1, word2, sort = TRUE)

bigram_counts <- filter(bigram_counts, !is.na(word1))

bigram_counts
```

```{r}
bigrams_united <- bigrams_filtered %>%
  unite(bigram, word1, word2, sep = " ")

bigrams_united
```

```{r}
bigram_graph <- bigram_counts %>%
  filter(n > 10) %>%
  graph_from_data_frame()

bigram_graph
```

```{r}
#overall bigram visualization
set.seed(2017)

ggraph(bigram_graph, layout = "fr") +
  geom_edge_link() +
  geom_node_point() +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1)
```

```{r}
#overall bigram visualization with directions
set.seed(2016)

a <- grid::arrow(type = "closed", length = unit(.15, "inches"))

ggraph(bigram_graph, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
                 arrow = a, end_cap = circle(.07, 'inches')) +
  geom_node_point(color = "lightblue", size = 5) +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
  theme_void()
```

```{r}
#prepare for visualization

install.packages("ggrepel") #`geom_label_repel`
install.packages("gridExtra") #`grid.arrange()` for multi-graphs
install.packages("knitr") #Create nicely formatted output tables
install.packages("kableExtra") #Create nicely formatted output tables
install.packages("formattable") #For the color_tile function
install.packages("circlize") #Visualizations - chord diagram
install.packages("memery") #Memes - images with plots
install.packages("magick") #Memes - images with plots (image_read)
install.packages("yarrr")  #Pirate plot
```

```{r}

#load the packages for visualization

library("ggrepel") #`geom_label_repel`
library("gridExtra") #`grid.arrange()` for multi-graphs
library("knitr") #Create nicely formatted output tables
library("kableExtra") #Create nicely formatted output tables
library("formattable") #For the color_tile function
library("circlize") #Visualizations - chord diagram
library("memery") #Memes - images with plots
library("magick") #Memes - images with plots (image_read)
library("yarrr")  #Pirate plot

```


```{r}

#Define some colors to use throughout

my_colors <- c("#E69F00", "#56B4E9", "#009E73", "#CC79A7", "#D55E00", "#D65E00")

#Customize ggplot2's default theme settings

theme_Y1Y4Clean <- function(aticks = element_blank(),
                         pgminor = element_blank(),
                         lt = element_blank(),
                         lp = "none")
{
  theme(plot.title = element_text(hjust = 0.5), #Center the title
        axis.ticks = aticks, #Set axis ticks to on or off
        panel.grid.minor = pgminor, #Turn the minor grid lines on or off$
        legend.title = lt, #Turn the legend title on or off
        legend.position = lp) #Turn the legend on or off
}

#Customize the text tables for consistency using HTML formatting
my_kable_styling <- function(dat, caption) {
  kable(dat, "html", escape = FALSE, caption = caption) %>%
  kable_styling(bootstrap_options = c("striped", "bordered"),
                full_width = FALSE)
}

```

##Sentiment Analysis

```{r}
#first sentiment analysis to figure out nrc (the function that sort words based on different sentiment)

new_sentiments <- sentiments %>% #From the tidytext package
  filter(lexicon != "loughran") %>% #Remove the finance lexicon
  mutate( sentiment = ifelse(lexicon == "AFINN" & score >= 0, "positive",
                              ifelse(lexicon == "AFINN" & score < 0,
                                     "negative", sentiment))) %>%
  group_by(lexicon) %>%
  mutate(words_in_lexicon = n_distinct(word)) %>%
  ungroup()

new_sentiments %>%
  group_by(lexicon, sentiment, words_in_lexicon) %>%
  summarise(distinct_words = n_distinct(word)) %>%
  ungroup() %>%
  spread(sentiment, distinct_words) %>%
  mutate(lexicon = color_tile("lightblue", "lightblue")(lexicon),
         words_in_lexicon = color_bar("lightpink")(words_in_lexicon)) %>%
  my_kable_styling(caption = "Word Counts Per Lexicon")

```


```{r}
# determine which lexicon is more applicable to the sentiment analysis
# check how many of those words are actually in the lexicons

Y1Y4Clean %>%
  mutate(words_in_Y1Y4Clean = n_distinct(word)) %>%
  inner_join(new_sentiments) %>%
  group_by(lexicon, words_in_Y1Y4Clean, words_in_lexicon) %>%
  summarise(lex_match_words = n_distinct(word)) %>%
  ungroup() %>%
  mutate(total_match_words = sum(lex_match_words), #Not used but good to have
         match_ratio = lex_match_words / words_in_Y1Y4Clean) %>%
  select(lexicon, lex_match_words,  words_in_Y1Y4Clean, match_ratio) %>%
  mutate(lex_match_words = color_bar("lightpink")(lex_match_words),
         lexicon = color_tile("lightgreen", "lightgreen")(lexicon)) %>%
  my_kable_styling(caption = "Y1Y4Clean Sentiment Found In Lexicons")

```

```{r}
#sample specific words sentiment analysis to show one word can represent different sentiments

new_sentiments %>%
  filter(word %in% c("patient", "patients", "treatment",
                     "plan", "expectation" , "feel" , "understand")) %>%
  arrange(word) %>% #sort

  mutate(word = color_tile("lightblue", "lightblue")(word),
         words_in_lexicon = color_bar("lightpink")(words_in_lexicon),
         lexicon = color_tile("lightgreen", "lightgreen")(lexicon)) %>%
  my_kable_styling(caption = "Specific Words")

```

```{r}
#create sentiment dataset

Y1Y4_bing <- Y1Y4Clean %>%
  inner_join(get_sentiments("bing"))

Y1Y4_nrc <- Y1Y4Clean %>%
  inner_join(get_sentiments("nrc"))

Y1Y4_nrc_sub <- Y1Y4Clean %>%
  inner_join(get_sentiments("nrc")) %>%
  filter(!sentiment %in% c("positive", "negative"))

```

```{r}
#in the mood: overall sentiment

nrc_plot <- Y1Y4_nrc %>%
  group_by(sentiment) %>%
  summarise(word_count = n()) %>%
  ungroup() %>%
  mutate(sentiment = reorder(sentiment, word_count)) %>%
  #Use `fill = -word_count` to make the larger bars darker
  ggplot(aes(sentiment, word_count, fill = word_count)) +
  geom_col() +
  guides(fill = FALSE) +
  theme_Y1Y4Clean()+
  labs(x = NULL, y = "Word_Count") +
  scale_y_continuous(limits = c(0, 5000)) +
  ggtitle("Y1Y4 NRC Sentiment") +
  coord_flip()


img <- "darkbackground.jpg" #Load the background image
lab <- ""  #Turn off the label
#Overlay the plot on the image and create the meme file
meme(img, lab, "meme_nrc.jpg", inset = nrc_plot)
#Read the file back in and display it!
nrc_meme <- image_read("meme_nrc.jpg")
plot(nrc_meme)


```

```{r}
#overall positive and negative

bing_plot <- Y1Y4_bing %>%
  group_by(sentiment) %>%
  summarise(word_count = n()) %>%
  ungroup() %>%
  mutate(sentiment = reorder(sentiment, word_count)) %>%
  ggplot(aes(sentiment, word_count, fill = sentiment)) +
  geom_col() +
  guides(fill = FALSE) +
  theme_Y1Y4Clean() +
  labs(x = NULL, y = "Word Count") +
  scale_y_continuous(limits = c(0, 2000)) +
  ggtitle("Y1Y4 Bing Sentiment") +
  coord_flip()

img1 <- "darkbackground.jpg"
lab1 <- ""
meme(img1, lab1, "meme_bing.jpg", inset = bing_plot)
x <- image_read("meme_bing.jpg")
plot(x)

```

```{r}
#Polarity changes and positive percentage changes over time

Y1Y4_polarity_year <- Y1Y4_bing %>%
  count(sentiment, RefYear) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(polarity = positive - negative,
    percent_positive = positive / (positive + negative) * 100)

Y1Y4_polarity_over_time <- Y1Y4_polarity_year %>%
  ggplot(aes(RefYear, polarity, color = ifelse(polarity >= 0,my_colors[5],my_colors[4]))) +
  geom_col() +
  geom_smooth(method = "loess", se = FALSE) +
  geom_smooth(method = "lm", se = FALSE, aes(color = my_colors[1])) +
  theme_Y1Y4Clean() + theme(plot.title = element_text(size = 11)) +
  xlab(NULL) + ylab(NULL) +
  ggtitle("Polarity Over Time")

Y1Y4_relative_polarity_over_time <- Y1Y4_polarity_year %>%
  ggplot(aes(RefYear, percent_positive , color = ifelse(polarity >= 0,my_colors[5],my_colors[4]))) +
  geom_col() +
  geom_smooth(method = "loess", se = FALSE) +
  geom_smooth(method = "lm", se = FALSE, aes(color = my_colors[1])) +
  theme_Y1Y4Clean() + theme(plot.title = element_text(size = 11)) +
  xlab(NULL) + ylab(NULL) +
  ggtitle("Percent Positive Over Time")

grid.arrange(Y1Y4_polarity_over_time, Y1Y4_relative_polarity_over_time, ncol = 2)

```

```{r}
#relationship between mood and year

grid.col = c("D1" = my_colors[1], "D4" = my_colors[2], "anger" = "grey", "anticipation" = "grey", "disgust" = "grey", "fear" = "grey", "joy" = "grey", "sadness" = "grey", "surprise" = "grey", "trust" = "grey")

Y1Y4_mood <-  Y1Y4_nrc %>%
  filter(RefYear != "NA" & !sentiment %in% c("positive", "negative")) %>%
  count(sentiment, RefYear) %>%
  group_by(RefYear, sentiment) %>%
  summarise(sentiment_sum = sum(n)) %>%
  ungroup()

circos.clear()
#Set the gap size
circos.par(gap.after = c(rep(5, length(unique(Y1Y4_mood[[1]])) - 1), 15,
                         rep(5, length(unique(Y1Y4_mood[[2]])) - 1), 15))
chordDiagram(Y1Y4_mood, grid.col = grid.col, transparency = .2)
title("Relationship Between Mood and Decade")



```

```{r}
#Y1Y4 NRC Sentiment, top 10 words in each sentiments

plot_words_D1_D4 <- Y1Y4_nrc %>%
  filter(RefYear %in% c("D1", "D4")) %>%
  group_by(sentiment) %>%
  count(word, sort = TRUE) %>%
  arrange(desc(n)) %>%
  slice(seq_len(8)) %>% #consider top_n() from dplyr also
  ungroup()

plot_words_D1_D4 %>%
  #Set `y = 1` to just plot one variable and use word as the label
  ggplot(aes(word, 1, label = word, fill = sentiment )) +
  #You want the words, not the points
  geom_point(color = "transparent") +
  #Make sure the labels don't overlap
  geom_label_repel(force = 1,nudge_y = .5,  
                   direction = "y",
                   box.padding = 0.04,
                   segment.color = "transparent",
                   size = 3) +
  facet_grid(~sentiment) +
  theme_Y1Y4Clean() +
  theme(axis.text.y = element_blank(), axis.text.x = element_blank(),
        axis.title.x = element_text(size = 6),
        panel.grid = element_blank(), panel.background = element_blank(),
        panel.border = element_rect("lightgray", fill = NA),
        strip.text.x = element_text(size = 9)) +
  xlab(NULL) + ylab(NULL) +
  ggtitle("Y1Y4 NRC Sentiment") +
  coord_flip()

```

```{r}
#Y1 NRC Sentiment, top 10 words in different sentiments

plot_words_Y1 <- Y1Y4_nrc %>%
  filter(RefYear == "D1") %>%
  group_by(sentiment) %>%
  count(word, sort = TRUE) %>%
  arrange(desc(n)) %>%
  slice(seq_len(10)) %>%
  ungroup()

#Same comments as previous graph
plot_words_Y1 %>%
  ggplot(aes(word, 1, label = word, fill = sentiment )) +
  geom_point(color = "transparent") +
  geom_label_repel(force = 1,nudge_y = .5,  
                   direction = "y",
                   box.padding = 0.05,
                   segment.color = "transparent",
                   size = 3) +
  facet_grid(~sentiment) +
  theme_Y1Y4Clean() +
  theme(axis.text.y = element_blank(), axis.text.x = element_blank(),
        axis.title.x = element_text(size = 6),
        panel.grid = element_blank(), panel.background = element_blank(),
        panel.border = element_rect("lightgray", fill = NA),
        strip.text.x = element_text(size = 9)) +
  xlab(NULL) + ylab(NULL) +
  ggtitle("Y1 NRC Sentiment") +
  coord_flip()


```

```{r}
#Y4 NRC Sentiment, top 10 words in different sentiments

plot_words_Y4 <- Y1Y4_nrc %>%
  filter(RefYear == "D4") %>%
  group_by(sentiment) %>%
  count(word, sort = TRUE) %>%
  arrange(desc(n)) %>%
  slice(seq_len(10)) %>%
  ungroup()

#Same comments as previous graph
plot_words_Y4 %>%
  ggplot(aes(word, 1, label = word, fill = sentiment )) +
  geom_point(color = "transparent") +
  geom_label_repel(force = 1,nudge_y = .5,  
                   direction = "y",
                   box.padding = 0.05,
                   segment.color = "transparent",
                   size = 3) +
  facet_grid(~sentiment) +
  theme_Y1Y4Clean() +
  theme(axis.text.y = element_blank(), axis.text.x = element_blank(),
        axis.title.x = element_text(size = 6),
        panel.grid = element_blank(), panel.background = element_blank(),
        panel.border = element_rect("lightgray", fill = NA),
        strip.text.x = element_text(size = 9)) +
  xlab(NULL) + ylab(NULL) +
  ggtitle("Y4 NRC Sentiment") +
  coord_flip()


```

```{r}
#Get the count of words per sentiment per year

year_sentiment_nrc <- Y1Y4_nrc_sub %>%
  group_by(RefYear, sentiment) %>%
  count(RefYear, sentiment) %>%
  select(RefYear, sentiment, sentiment_RefYear_count = n)

#Get the total count of sentiment words per year (not distinct)
total_sentiment_year <- Y1Y4_nrc_sub %>%
  count(RefYear) %>%
  select(RefYear, RefYear_total = n)

install.packages("chartJSRadar")
library(chartJSRadar)
#Join the two and create a percent field
year_radar_chart <- year_sentiment_nrc %>%
  inner_join(total_sentiment_year, by = "RefYear") %>%
  mutate(percent = sentiment_RefYear_count / RefYear_total * 100 ) %>%
  filter(RefYear %in% c("D1","D4")) %>%
  select(-sentiment_RefYear_count, -RefYear_total) %>%
  spread(RefYear, percent) %>%
  chartJSRadar(showToolTipLabel = TRUE,
               main = "NRC Years Radar")

```
##[Q1]Bigram
###Y1Q1-bigram
```{r}
#[Y1Q1] 
#bigram count
Y1Q1_bigrams <- Y1Q1 %>%
  unnest_tokens(bigram, word, token = "ngrams", n = 2)

Y1Q1_bigrams %>%
count(bigram, sort = TRUE)

#overall bigram count that filtered out stop words
bigrams_separated_Y1Q1 <- Y1Q1_bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ")

bigrams_filtered_Y1Q1 <- bigrams_separated_Y1Q1 %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)

#new bigram counts:
bigram_counts_Y1Q1 <- bigrams_filtered_Y1Q1 %>% 
  count(word1, word2, sort = TRUE)

bigram_counts_Y1Q1 <- filter(bigram_counts_Y1Q1, !is.na(word1))


bigram_counts_Y1Q1

bigrams_united_Y1Q1 <- bigrams_filtered_Y1Q1 %>%
  unite(bigram, word1, word2, sep = " ")

bigrams_united_Y1Q1

bigram_graph_Y1Q1 <- bigram_counts_Y1Q1 %>%
  filter(n > 10) %>%
  graph_from_data_frame()

bigram_graph_Y1Q1

#overall bigram visualization
set.seed(2017)

ggraph(bigram_graph_Y1Q1, layout = "fr") +
  geom_edge_link() +
  geom_node_point() +
geom_node_text(aes(label = name), vjust = 1, hjust = 1)

#overall bigram visualization with directions
set.seed(2016)

a <- grid::arrow(type = "closed", length = unit(.15, "inches"))

ggraph(bigram_graph_Y1Q1, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
                 arrow = a, end_cap = circle(.07, 'inches')) +
  geom_node_point(color = "lightblue", size = 5) +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
  theme_void()



```

###Y4Q1-bigram
```{r}
#overall bigram count
Y4Q1_bigrams <- Y4Q1 %>%
  unnest_tokens(bigram, word, token = "ngrams", n = 2)

Y4Q1_bigrams %>%
count(bigram, sort = TRUE)

#overall bigram count that filtered out stop words
bigrams_separated_Y4Q1 <- Y4Q1_bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ")

bigrams_filtered_Y4Q1 <- bigrams_separated_Y4Q1 %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)

#new bigram counts:
bigram_counts_Y4Q1 <- bigrams_filtered_Y4Q1 %>% 
  count(word1, word2, sort = TRUE)

bigram_counts_Y1Q1 <- filter(bigram_counts_Y4Q1, !is.na(word1))


bigram_counts_Y4Q1

bigrams_united_Y4Q1 <- bigrams_filtered_Y4Q1 %>%
  unite(bigram, word1, word2, sep = " ")

bigrams_united_Y4Q1

bigram_graph_Y4Q1 <- bigram_counts_Y4Q1 %>%
  filter(n > 10) %>%
  graph_from_data_frame()

bigram_graph_Y4Q1

#overall bigram visualization
set.seed(2017)

ggraph(bigram_graph_Y4Q1, layout = "fr") +
  geom_edge_link() +
  geom_node_point() +
geom_node_text(aes(label = name), vjust = 1, hjust = 1)

#overall bigram visualization with directions
set.seed(2016)

a <- grid::arrow(type = "closed", length = unit(.15, "inches"))

ggraph(bigram_graph_Y4Q1, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
                 arrow = a, end_cap = circle(.07, 'inches')) +
  geom_node_point(color = "lightblue", size = 5) +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
  theme_void()


```

##[Q2]Bigram
###Y1Q2-bigram
```{r}
#[Y1Q2] 
#bigram count
Y1Q2_bigrams <- Y1Q2 %>%
  unnest_tokens(bigram, word, token = "ngrams", n = 2)

Y1Q2_bigrams %>%
count(bigram, sort = TRUE)

#overall bigram count that filtered out stop words
bigrams_separated_Y1Q2 <- Y1Q2_bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ")

bigrams_filtered_Y1Q2 <- bigrams_separated_Y1Q2 %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)

#new bigram counts:
bigram_counts_Y1Q2 <- bigrams_filtered_Y1Q2 %>% 
  count(word1, word2, sort = TRUE)

bigram_counts_Y1Q2 <- filter(bigram_counts_Y1Q2, !is.na(word1))


bigram_counts_Y1Q2

bigrams_united_Y1Q2 <- bigrams_filtered_Y1Q2 %>%
  unite(bigram, word1, word2, sep = " ")

bigrams_united_Y1Q2

bigram_graph_Y1Q2 <- bigram_counts_Y1Q2 %>%
  filter(n > 10) %>%
  graph_from_data_frame()

bigram_graph_Y1Q2

#overall bigram visualization
set.seed(2017)

ggraph(bigram_graph_Y1Q2, layout = "fr") +
  geom_edge_link() +
  geom_node_point() +
geom_node_text(aes(label = name), vjust = 1, hjust = 1)

#overall bigram visualization with directions
set.seed(2016)

a <- grid::arrow(type = "closed", length = unit(.15, "inches"))

ggraph(bigram_graph_Y1Q2, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
                 arrow = a, end_cap = circle(.07, 'inches')) +
  geom_node_point(color = "lightblue", size = 5) +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
  theme_void()

```

###Y4Q2-bigram
```{r}
#[Y4Q2] 
#bigram count
Y4Q2_bigrams <- Y4Q2 %>%
  unnest_tokens(bigram, word, token = "ngrams", n = 2)

Y4Q2_bigrams %>%
count(bigram, sort = TRUE)

#overall bigram count that filtered out stop words
bigrams_separated_Y4Q2 <- Y4Q2_bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ")

bigrams_filtered_Y4Q2 <- bigrams_separated_Y4Q2 %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)

#new bigram counts:
bigram_counts_Y4Q2 <- bigrams_filtered_Y4Q2 %>% 
  count(word1, word2, sort = TRUE)

bigram_counts_Y4Q2 <- filter(bigram_counts_Y4Q2, !is.na(word1))


bigram_counts_Y4Q2

bigrams_united_Y4Q2 <- bigrams_filtered_Y4Q2 %>%
  unite(bigram, word1, word2, sep = " ")

bigrams_united_Y4Q2

bigram_graph_Y4Q2 <- bigram_counts_Y4Q2 %>%
  filter(n > 10) %>%
  graph_from_data_frame()

bigram_graph_Y4Q2

#overall bigram visualization
set.seed(2017)

ggraph(bigram_graph_Y4Q2, layout = "fr") +
  geom_edge_link() +
  geom_node_point() +
geom_node_text(aes(label = name), vjust = 1, hjust = 1)

#overall bigram visualization with directions
set.seed(2016)

a <- grid::arrow(type = "closed", length = unit(.15, "inches"))

ggraph(bigram_graph_Y4Q2, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
                 arrow = a, end_cap = circle(.07, 'inches')) +
  geom_node_point(color = "lightblue", size = 5) +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
  theme_void()

```

##[Q1]SentimentTop20
```{r}
#Q1--Y1 NRC Sentiment, top 20 words in different sentiments

plot_words_Y1Q1 <- Y1Y4_nrc %>%
  filter(RefYear == "D1") %>%
  filter(kiqQs == "cp3") %>%
  group_by(sentiment) %>%
  count(word, sort = TRUE) %>%
  arrange(desc(n)) %>%
  slice(seq_len(20)) %>%
  ungroup()

#Same comments as previous graph
plot_words_Y1Q1 %>%
  ggplot(aes(word, 1, label = word, fill = sentiment )) +
  geom_point(color = "transparent") +
  geom_label_repel(force = 1,nudge_y = .5,  
                   direction = "y",
                   box.padding = 0.05,
                   segment.color = "transparent",
                   size = 3) +
  facet_grid(~sentiment) +
  theme_Y1Y4Clean() +
  theme(axis.text.y = element_blank(), axis.text.x = element_blank(),
        axis.title.x = element_text(size = 4),
        panel.grid = element_blank(), panel.background = element_blank(),
        panel.border = element_rect("lightgray", fill = NA),
        strip.text.x = element_text(size = 7)) +
  xlab(NULL) + ylab(NULL) +
  ggtitle("Y1Q1 NRC Sentiment") +
  coord_flip()

#Q1--Y4 NRC Sentiment, top 20 words in different sentiments

plot_words_Y4Q1 <- Y1Y4_nrc %>%
  filter(RefYear == "D4") %>%
  filter(kiqQs == "cp3") %>%
  group_by(sentiment) %>%
  count(word, sort = TRUE) %>%
  arrange(desc(n)) %>%
  slice(seq_len(20)) %>%
  ungroup()

#Same comments as previous graph
plot_words_Y4Q1 %>%
  ggplot(aes(word, 1, label = word, fill = sentiment )) +
  geom_point(color = "transparent") +
  geom_label_repel(force = 1,nudge_y = .5,  
                   direction = "y",
                   box.padding = 0.05,
                   segment.color = "transparent",
                   size = 3) +
  facet_grid(~sentiment) +
  theme_Y1Y4Clean() +
  theme(axis.text.y = element_blank(), axis.text.x = element_blank(),
        axis.title.x = element_text(size = 4),
        panel.grid = element_blank(), panel.background = element_blank(),
        panel.border = element_rect("lightgray", fill = NA),
        strip.text.x = element_text(size = 7)) +
  xlab(NULL) + ylab(NULL) +
  ggtitle("Y4Q1 NRC Sentiment") +
  coord_flip()
```

##[Q2]SentimentTop20
```{r}
#Q2--Y1 NRC Sentiment, top 20 words in different sentiments

plot_words_Y1Q2 <- Y1Y4_nrc %>%
  filter(RefYear == "D1") %>%
  filter(kiqQs == "cp22") %>%
  group_by(sentiment) %>%
  count(word, sort = TRUE) %>%
  arrange(desc(n)) %>%
  slice(seq_len(20)) %>%
  ungroup()

#Same comments as previous graph
plot_words_Y1Q2 %>%
  ggplot(aes(word, 1, label = word, fill = sentiment )) +
  geom_point(color = "transparent") +
  geom_label_repel(force = 1,nudge_y = .5,  
                   direction = "y",
                   box.padding = 0.05,
                   segment.color = "transparent",
                   size = 3) +
  facet_grid(~sentiment) +
  theme_Y1Y4Clean() +
  theme(axis.text.y = element_blank(), axis.text.x = element_blank(),
        axis.title.x = element_text(size = 4),
        panel.grid = element_blank(), panel.background = element_blank(),
        panel.border = element_rect("lightgray", fill = NA),
        strip.text.x = element_text(size = 7)) +
  xlab(NULL) + ylab(NULL) +
  ggtitle("Y1Q2 NRC Sentiment") +
  coord_flip()

#Q2--Y4 NRC Sentiment, top 20 words in different sentiments

plot_words_Y4Q2 <- Y1Y4_nrc %>%
  filter(RefYear == "D4") %>%
  filter(kiqQs == "cp22") %>%
  group_by(sentiment) %>%
  count(word, sort = TRUE) %>%
  arrange(desc(n)) %>%
  slice(seq_len(20)) %>%
  ungroup()

#Same comments as previous graph
plot_words_Y4Q2 %>%
  ggplot(aes(word, 1, label = word, fill = sentiment )) +
  geom_point(color = "transparent") +
  geom_label_repel(force = 1,nudge_y = .5,  
                   direction = "y",
                   box.padding = 0.05,
                   segment.color = "transparent",
                   size = 3) +
  facet_grid(~sentiment) +
  theme_Y1Y4Clean() +
  theme(axis.text.y = element_blank(), axis.text.x = element_blank(),
        axis.title.x = element_text(size = 4),
        panel.grid = element_blank(), panel.background = element_blank(),
        panel.border = element_rect("lightgray", fill = NA),
        strip.text.x = element_text(size = 7)) +
  xlab(NULL) + ylab(NULL) +
  ggtitle("Y4Q2 NRC Sentiment") +
  coord_flip()
```

##[Q1]WeightedSentimentRelationships
```{r}
library(ggplot2)
library(igraph)
library(ggraph)

##Q1--Y1
set.seed(1234)

plot_words_Y1Q1 <- Y1Y4_nrc %>%
  filter(RefYear == "D1") %>%
  filter(kiqQs == "cp3") %>%
  group_by(sentiment) %>%
  count(word, sort = TRUE) %>%
  arrange(desc(n)) %>%
  slice(seq_len(20)) %>%
  ungroup()

plot_words_Y1Q1 %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(edge_alpha = n, edge_width = n), edge_colour = "cyan4") +
  geom_node_point(size = 1) +
  geom_node_text(aes(label = name), repel = TRUE, 
                 point.padding = unit(0.2, "lines")) +
  theme_void()

##Q1--Y4
set.seed(1234)

plot_words_Y4Q1 <- Y1Y4_nrc %>%
  filter(RefYear == "D4") %>%
  filter(kiqQs == "cp3") %>%
  group_by(sentiment) %>%
  count(word, sort = TRUE) %>%
  arrange(desc(n)) %>%
  slice(seq_len(20)) %>%
  ungroup()

plot_words_Y4Q1 %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(edge_alpha = n, edge_width = n), edge_colour = "cyan4") +
  geom_node_point(size = 1) +
  geom_node_text(aes(label = name), repel = TRUE, 
                 point.padding = unit(0.2, "lines")) +
  theme_void()
```

##[Q2]WeightedSentimentRelationships
```{r}
##Q2--Y1
set.seed(1234)

plot_words_Y1Q2 <- Y1Y4_nrc %>%
  filter(RefYear == "D1") %>%
  filter(kiqQs == "cp22") %>%
  group_by(sentiment) %>%
  count(word, sort = TRUE) %>%
  arrange(desc(n)) %>%
  slice(seq_len(20)) %>%
  ungroup()

plot_words_Y1Q2 %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(edge_alpha = n, edge_width = n), edge_colour = "cyan4") +
  geom_node_point(size = 1) +
  geom_node_text(aes(label = name), repel = TRUE, 
                 point.padding = unit(0.2, "lines")) +
  theme_void()

##Q2--Y4
set.seed(1234)

plot_words_Y4Q2 <- Y1Y4_nrc %>%
  filter(RefYear == "D4") %>%
  filter(kiqQs == "cp22") %>%
  group_by(sentiment) %>%
  count(word, sort = TRUE) %>%
  arrange(desc(n)) %>%
  slice(seq_len(20)) %>%
  ungroup()

plot_words_Y4Q2 %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(edge_alpha = n, edge_width = n), edge_colour = "cyan4") +
  geom_node_point(size = 1) +
  geom_node_text(aes(label = name), repel = TRUE, 
                 point.padding = unit(0.2, "lines")) +
  theme_void()
```


```{r}
#Doneeeeeeeeeeeeeee!
#Great job guys!














